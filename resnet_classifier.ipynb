{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97fc4b32-831e-4609-abdd-40a8dc1d29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import adam_v2\n",
    "import urllib.request\n",
    "import ssl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6ae94a-e6a1-49eb-a18b-64f1a185d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input image size\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# Set the number of epochs and batch size\n",
    "epochs = 20\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c605cbfc-436e-477c-b058-ae3a0bb26fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the dataset \n",
    "train_dir = '/Volumes/SSD/dishes'\n",
    "valid_dir = '/Volumes/SSD/dishes-val'\n",
    "test_dir = '/Volumes/SSD/dishes-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f17eed-4e5a-4991-b1ab-e7188ed7f684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1985 images belonging to 2 classes.\n",
      "Found 833 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create the data generators\n",
    "# Create a data generator for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# Create a data generator for the validation set\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary')\n",
    "valid_generator = valid_datagen.flow_from_directory(valid_dir, target_size=(img_width, img_height), batch_size=batch_size, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ad6054-55da-4544-951d-09a182da0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable SSL certificate verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3847de9b-04b2-4135-905e-ba060f7edb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the resnet50 weights manually\n",
    "weights_path = 'resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "if not os.path.exists(weights_path):\n",
    "    url = 'https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    urllib.request.urlretrieve(url, weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310f154f-c55d-422c-8a49-ead48e4a4a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 13:42:03.724312: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the ResNet50 model pre-trained on ImageNet and remove the top layer\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e074068b-be96-4582-b9e7-d3344d7ab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a global average pooling layer and a dense layer with sigmoid activation for binary classification\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29da97f9-f74a-4af3-9da3-74b0467a3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer=adam_v2.Adam(learning_rate=0.0001, decay=0.0001/epochs), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae6befde-c597-47ef-8ebd-29fb54696d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "63/63 [==============================] - 980s 15s/step - loss: 0.4135 - accuracy: 0.7985 - val_loss: 1.9678 - val_accuracy: 0.4682\n",
      "Epoch 2/20\n",
      "63/63 [==============================] - 914s 15s/step - loss: 0.1545 - accuracy: 0.9416 - val_loss: 0.8716 - val_accuracy: 0.4694\n",
      "Epoch 3/20\n",
      "63/63 [==============================] - 914s 15s/step - loss: 0.0797 - accuracy: 0.9698 - val_loss: 0.7301 - val_accuracy: 0.4682\n",
      "Epoch 4/20\n",
      "63/63 [==============================] - 916s 15s/step - loss: 0.0825 - accuracy: 0.9733 - val_loss: 0.7577 - val_accuracy: 0.5282\n",
      "Epoch 5/20\n",
      "63/63 [==============================] - 913s 14s/step - loss: 0.0601 - accuracy: 0.9758 - val_loss: 1.2611 - val_accuracy: 0.5282\n",
      "Epoch 6/20\n",
      "63/63 [==============================] - 915s 15s/step - loss: 0.0606 - accuracy: 0.9788 - val_loss: 0.7647 - val_accuracy: 0.5030\n",
      "Epoch 7/20\n",
      "63/63 [==============================] - 914s 15s/step - loss: 0.0291 - accuracy: 0.9919 - val_loss: 0.7957 - val_accuracy: 0.5246\n",
      "Epoch 8/20\n",
      "63/63 [==============================] - 939s 15s/step - loss: 0.0555 - accuracy: 0.9773 - val_loss: 0.9164 - val_accuracy: 0.4994\n",
      "Epoch 9/20\n",
      "63/63 [==============================] - 936s 15s/step - loss: 0.0798 - accuracy: 0.9718 - val_loss: 3.2216 - val_accuracy: 0.4970\n",
      "Epoch 10/20\n",
      "63/63 [==============================] - 934s 15s/step - loss: 0.0507 - accuracy: 0.9839 - val_loss: 1.1042 - val_accuracy: 0.6074\n",
      "Epoch 11/20\n",
      "63/63 [==============================] - 933s 15s/step - loss: 0.0201 - accuracy: 0.9945 - val_loss: 0.9972 - val_accuracy: 0.6291\n",
      "Epoch 12/20\n",
      "63/63 [==============================] - 928s 15s/step - loss: 0.0365 - accuracy: 0.9884 - val_loss: 0.9876 - val_accuracy: 0.6255\n",
      "Epoch 13/20\n",
      "63/63 [==============================] - 928s 15s/step - loss: 0.0391 - accuracy: 0.9884 - val_loss: 1.3864 - val_accuracy: 0.6591\n",
      "Epoch 14/20\n",
      "63/63 [==============================] - 930s 15s/step - loss: 0.0500 - accuracy: 0.9854 - val_loss: 1.8264 - val_accuracy: 0.6795\n",
      "Epoch 15/20\n",
      "63/63 [==============================] - 929s 15s/step - loss: 0.0409 - accuracy: 0.9849 - val_loss: 1.2640 - val_accuracy: 0.7347\n",
      "Epoch 16/20\n",
      "63/63 [==============================] - 928s 15s/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 1.1555 - val_accuracy: 0.7299\n",
      "Epoch 17/20\n",
      "63/63 [==============================] - 10500s 169s/step - loss: 0.0281 - accuracy: 0.9914 - val_loss: 1.4111 - val_accuracy: 0.7359\n",
      "Epoch 18/20\n",
      "63/63 [==============================] - 935s 15s/step - loss: 0.0481 - accuracy: 0.9814 - val_loss: 0.8308 - val_accuracy: 0.7959\n",
      "Epoch 19/20\n",
      "63/63 [==============================] - 900s 14s/step - loss: 0.0562 - accuracy: 0.9819 - val_loss: 1.1054 - val_accuracy: 0.7719\n",
      "Epoch 20/20\n",
      "63/63 [==============================] - 893s 14s/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 1.2269 - val_accuracy: 0.7875\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_generator, validation_data=valid_generator, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac27fce-71a6-4331-b684-889ee1b684a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2321 images belonging to 2 classes.\n",
      "73/73 [==============================] - 232s 3s/step - loss: 904.0524 - accuracy: 0.5536\n",
      "Test accuracy: 0.5536406636238098\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87602499-fbde-4b8c-a9bd-2b07f071cdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
